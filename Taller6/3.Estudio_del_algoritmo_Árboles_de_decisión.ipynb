{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLlyu4EviY9E"
      },
      "source": [
        "Johana Tellez-Michael Caicedo\n",
        "##Descripción general del algoritmo\n",
        "\n",
        "Los árboles de decisión son modelos supervisados que aprenden reglas jerárquicas para clasificar datos. A partir de los atributos, dividen recursivamente el espacio en regiones más homogéneas, generando reglas del tipo:\n",
        "\n",
        "“Si player_pos <= 3 y ball_x >= 7, entonces acción = Arriba”\n",
        "\n",
        "Su ventaja principal es la interpretabilidad, ya que las reglas resultantes pueden leerse como decisiones lógicas. También son rápidos en predicción y pueden generalizar bien si se controlan la profundidad y el tamaño mínimo de hojas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nafc0Lu-jIGG"
      },
      "source": [
        "#1. Definición de PongEnvironment y PongAgent\n",
        "\n",
        "La clase PongEnvironment define el entorno del juego Pong, incluyendo el tablero, la posición de la pelota y del jugador, las reglas, las recompensas y las penalizaciones. Se encarga de actualizar el estado del juego en cada paso y de detectar colisiones, puntos y finales de partida.\n",
        "\n",
        "La clase PongAgent representa al agente que aprende a jugar. Contiene la tabla Q, donde se almacenan las políticas aprendidas para cada estado y acción. Además, implementa los métodos para elegir acciones (explorando o explotando) y actualizar la tabla Q usando la ecuación de Bellman en función de las recompensas recibidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KMYVAljHiXol"
      },
      "outputs": [],
      "source": [
        "# ==============================================\n",
        "# 1. Definición de PongEnvironment y PongAgent\n",
        "# ==============================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "from math import ceil, floor\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class PongEnvironment:\n",
        "    def __init__(self, max_life=3, height_px = 40, width_px = 50, movimiento_px = 3):\n",
        "        self.action_space = ['Arriba','Abajo']\n",
        "        self._step_penalization = 0\n",
        "        self.state = [0,0,0]\n",
        "        self.total_reward = 0\n",
        "        self.dx = movimiento_px\n",
        "        self.dy = movimiento_px\n",
        "        filas = ceil(height_px/movimiento_px)\n",
        "        columnas = ceil(width_px/movimiento_px)\n",
        "        self.positions_space = np.array([[[0 for z in range(columnas)]\n",
        "                                                  for y in range(filas)]\n",
        "                                                     for x in range(filas)])\n",
        "        self.lives = max_life\n",
        "        self.max_life = max_life\n",
        "        self.x = randint(int(width_px/2), width_px)\n",
        "        self.y = randint(0, height_px-10)\n",
        "        self.player_alto = int(height_px/4)\n",
        "        self.player1 = self.player_alto\n",
        "        self.score = 0\n",
        "        self.width_px = width_px\n",
        "        self.height_px = height_px\n",
        "        self.radio = 2.5\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_reward = 0\n",
        "        self.state = [0,0,0]\n",
        "        self.lives = self.max_life\n",
        "        self.score = 0\n",
        "        self.x = randint(int(self.width_px/2), self.width_px)\n",
        "        self.y = randint(0, self.height_px-10)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action, animate=False):\n",
        "        self._apply_action(action, animate)\n",
        "        done = self.lives <= 0\n",
        "        reward = self.score + self._step_penalization\n",
        "        self.total_reward += reward\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def _apply_action(self, action, animate=False):\n",
        "        if action == \"Arriba\":\n",
        "            self.player1 += abs(self.dy)\n",
        "        elif action == \"Abajo\":\n",
        "            self.player1 -= abs(self.dy)\n",
        "        self.avanza_player()\n",
        "        self.avanza_frame()\n",
        "        self.state = (floor(self.player1/abs(self.dy))-2,\n",
        "                      floor(self.y/abs(self.dy))-2,\n",
        "                      floor(self.x/abs(self.dx))-2)\n",
        "\n",
        "    def detectaColision(self, ball_y, player_y):\n",
        "        return (player_y+self.player_alto >= (ball_y-self.radio)) and (player_y <= (ball_y+self.radio))\n",
        "\n",
        "    def avanza_player(self):\n",
        "        if self.player1 + self.player_alto >= self.height_px:\n",
        "            self.player1 = self.height_px - self.player_alto\n",
        "        elif self.player1 <= -abs(self.dy):\n",
        "            self.player1 = -abs(self.dy)\n",
        "\n",
        "    def avanza_frame(self):\n",
        "        self.x += self.dx\n",
        "        self.y += self.dy\n",
        "        if self.x <= 3 or self.x > self.width_px:\n",
        "            self.dx = -self.dx\n",
        "            if self.x <= 3:\n",
        "                ret = self.detectaColision(self.y, self.player1)\n",
        "                if ret:\n",
        "                    self.score = 10\n",
        "                else:\n",
        "                    self.score = -10\n",
        "                    self.lives -= 1\n",
        "                    if self.lives > 0:\n",
        "                        self.x = randint(int(self.width_px/2), self.width_px)\n",
        "                        self.y = randint(0, self.height_px-10)\n",
        "                        self.dx = abs(self.dx)\n",
        "                        self.dy = abs(self.dy)\n",
        "        else:\n",
        "            self.score = 0\n",
        "        if self.y < 0 or self.y > self.height_px:\n",
        "            self.dy = -self.dy\n",
        "\n",
        "class PongAgent:\n",
        "    def __init__(self, game, policy=None, discount_factor = 0.1, learning_rate = 0.1, ratio_explotacion = 0.9):\n",
        "        if policy is not None:\n",
        "            self._q_table = policy\n",
        "        else:\n",
        "            position = list(game.positions_space.shape)\n",
        "            position.append(len(game.action_space))\n",
        "            self._q_table = np.zeros(position)\n",
        "        self.discount_factor = discount_factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.ratio_explotacion = ratio_explotacion\n",
        "\n",
        "    def get_next_step(self, state, game):\n",
        "        next_step = np.random.choice(list(game.action_space))\n",
        "        if np.random.uniform() <= self.ratio_explotacion:\n",
        "            idx_action = np.random.choice(np.flatnonzero(\n",
        "                self._q_table[state[0],state[1],state[2]] ==\n",
        "                self._q_table[state[0],state[1],state[2]].max()\n",
        "            ))\n",
        "            next_step = list(game.action_space)[idx_action]\n",
        "        return next_step\n",
        "\n",
        "    def update(self, game, old_state, action_taken, reward_action_taken, new_state, reached_end):\n",
        "        idx_action_taken = list(game.action_space).index(action_taken)\n",
        "        actual_q_value = self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken]\n",
        "        future_max_q_value = reward_action_taken + self.discount_factor * self._q_table[new_state[0], new_state[1], new_state[2]].max()\n",
        "        if reached_end:\n",
        "            future_max_q_value = reward_action_taken\n",
        "        self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken] = actual_q_value + self.learning_rate*(future_max_q_value - actual_q_value)\n",
        "\n",
        "    def get_policy(self):\n",
        "        return self._q_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghFqSMnFun1x"
      },
      "source": [
        "##2. Función de entrenamiento\n",
        "La función de entrenamiento (play) se encarga de simular múltiples partidas de Pong para que el agente aprenda por refuerzo. Durante cada episodio, el agente interactúa con el entorno, elige acciones, recibe recompensas y actualiza su tabla Q para mejorar sus decisiones futuras.\n",
        "\n",
        "La función controla el número de iteraciones, acumula estadísticas como puntajes y pasos promedio, y ajusta las políticas del agente con base en la experiencia acumulada, permitiendo que este evolucione desde un comportamiento aleatorio hasta una estrategia más eficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "32KWfVuAuopr"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 2. Función de entrenamiento\n",
        "# ============================\n",
        "def play(rounds=5000, max_life=3, discount_factor = 0.1, learning_rate = 0.1, ratio_explotacion=0.9, learner=None, game=None):\n",
        "    if game is None:\n",
        "        game = PongEnvironment(max_life=max_life, movimiento_px=3)\n",
        "    if learner is None:\n",
        "        learner = PongAgent(game, discount_factor=discount_factor, learning_rate=learning_rate, ratio_explotacion=ratio_explotacion)\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        state = game.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            old_state = np.array(state)\n",
        "            action = learner.get_next_step(state, game)\n",
        "            state, reward, done = game.step(action)\n",
        "            learner.update(game, old_state, action, reward, state, done)\n",
        "    return learner, game\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz0iNCSSu6CA"
      },
      "source": [
        "## 3. Entrenamiento + Árbol de decisión\n",
        "En esta etapa se utiliza la tabla Q entrenada por el agente para construir un dataset supervisado, donde cada estado del juego (posición del jugador y de la pelota) actúa como entrada, y la mejor acción aprendida corresponde a la etiqueta.\n",
        "\n",
        "Con este dataset, se entrena un modelo de Árbol de Decisión para que aprenda a imitar la política del agente. Luego, el modelo es evaluado en un conjunto de validación independiente, calculando métricas como precisión y recall. Esto permite verificar qué tan bien el árbol logra reproducir las decisiones del agente, facilitando su uso posterior sin necesidad de ejecutar Q-Learning en tiempo real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VKwXbTwu6cd",
        "outputId": "bcb4686b-b436-47a9-de36-804d2b85a8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exactitud Árbol de Decisión: 0.6626686656671664\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.71      0.70       378\n",
            "           1       0.61      0.61      0.61       289\n",
            "\n",
            "    accuracy                           0.66       667\n",
            "   macro avg       0.66      0.66      0.66       667\n",
            "weighted avg       0.66      0.66      0.66       667\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# 3. Entrenamiento + Árbol de decisión\n",
        "# ===============================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Entrenar agente y obtener política\n",
        "learner, game = play(rounds=5000)\n",
        "\n",
        "q_table = learner.get_policy()\n",
        "pos_n, y_n, x_n, n_actions = q_table.shape\n",
        "\n",
        "# Dataset\n",
        "X, y = [], []\n",
        "for i in range(pos_n):\n",
        "    for j in range(y_n):\n",
        "        for k in range(x_n):\n",
        "            best_action_idx = int(np.argmax(q_table[i, j, k]))\n",
        "            X.append([i, j, k])\n",
        "            y.append(best_action_idx)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Árbol de decisión\n",
        "tree_model = DecisionTreeClassifier(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = tree_model.predict(X_val)\n",
        "print(\"Exactitud Árbol de Decisión:\", accuracy_score(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9tHfqfcjS2b"
      },
      "source": [
        "##Intrepretación\n",
        "El modelo de árbol de decisión obtuvo una exactitud del 66,3 %, lo que indica que aproximadamente dos de cada tres predicciones coinciden con las acciones reales definidas en la tabla Q.\n",
        "\n",
        "Para la clase 0 (acción “Arriba”), se logró una precisión de 0.70 y un recall de 0.71, mostrando un buen desempeño en la identificación de esta acción. En la clase 1 (acción “Abajo”), la precisión y el recall son de 0.61, lo que revela un comportamiento más modesto, posiblemente por diferencias en la distribución de ejemplos o mayor complejidad en la toma de decisiones para esa clase.\n",
        "\n",
        "En general, el árbol logra capturar correctamente la estrategia del agente en la mayoría de los casos, mostrando un balance razonable entre ambas clases. Sin embargo, aún existe espacio para mejorar mediante ajustes en los hiperparámetros o técnicas de optimización para afinar la generalización del modelo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
