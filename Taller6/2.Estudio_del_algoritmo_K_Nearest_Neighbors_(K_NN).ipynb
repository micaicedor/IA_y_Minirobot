{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuQ-3xNfgtU-"
      },
      "source": [
        "Johana Tellez-Michael Caicedo\n",
        "##Descripción general del algoritmo\n",
        "\n",
        "El algoritmo K-Nearest Neighbors (K-NN) es un método de clasificación basado en vecindad. Para clasificar un nuevo punto, busca los k ejemplos más cercanos en el conjunto de entrenamiento y asigna la clase más común entre ellos.\n",
        "Es un modelo no paramétrico (no aprende una función explícita), simple de implementar y efectivo cuando la estructura espacial de los datos refleja las clases.\n",
        "\n",
        "En este contexto, utilizaremos K-NN para aproximar la política aprendida por el agente Q-Learning. La idea es extraer pares (estado, mejor acción) desde la tabla Q entrenada, y entrenar un K-NN que imite esas decisiones. Luego, el modelo puede sustituir o complementar a la política original en la toma de decisiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYk8_Xskg2_1"
      },
      "source": [
        "##1. CLASES DEL ENTORNO Y AGENTE\n",
        "\n",
        "En esta sección se definen las estructuras fundamentales del experimento de aprendizaje por refuerzo:\n",
        "\n",
        "PongEnvironment: es la clase que simula el juego de Pong. Define el espacio de estados (posición del jugador y de la pelota), las acciones posibles (mover arriba o abajo), las reglas del juego (rebotes, colisiones, pérdida de vidas) y entrega las recompensas en cada paso.\n",
        "\n",
        "PongAgent: representa al agente que aprende a jugar. Contiene la tabla Q (donde guarda su política), el método para decidir acciones (get_next_step) y la función para actualizar su conocimiento mediante la ecuación de Bellman (update)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tj_M6mLXgkJY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "from math import ceil, floor\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ==========================\n",
        "# 1. CLASES DEL ENTORNO Y AGENTE\n",
        "# ==========================\n",
        "class PongEnvironment:\n",
        "    def __init__(self, max_life=3, height_px=40, width_px=50, movimiento_px=3):\n",
        "        self.action_space = ['Arriba', 'Abajo']\n",
        "        self._step_penalization = 0\n",
        "        self.state = [0,0,0]\n",
        "        self.total_reward = 0\n",
        "        self.dx = movimiento_px\n",
        "        self.dy = movimiento_px\n",
        "        filas = ceil(height_px/movimiento_px)\n",
        "        columnas = ceil(width_px/movimiento_px)\n",
        "        self.positions_space = np.array([[[0 for z in range(columnas)] for y in range(filas)] for x in range(filas)])\n",
        "        self.lives = max_life\n",
        "        self.max_life = max_life\n",
        "        self.x = randint(int(width_px/2), width_px)\n",
        "        self.y = randint(0, height_px-10)\n",
        "        self.player_alto = int(height_px/4)\n",
        "        self.player1 = self.player_alto\n",
        "        self.score = 0\n",
        "        self.width_px = width_px\n",
        "        self.height_px = height_px\n",
        "        self.radio = 2.5\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_reward = 0\n",
        "        self.state = [0,0,0]\n",
        "        self.lives = self.max_life\n",
        "        self.score = 0\n",
        "        self.x = randint(int(self.width_px/2), self.width_px)\n",
        "        self.y = randint(0, self.height_px-10)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action, animate=False):\n",
        "        self._apply_action(action, animate)\n",
        "        done = self.lives <= 0\n",
        "        reward = self.score + self._step_penalization\n",
        "        self.total_reward += reward\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def _apply_action(self, action, animate=False):\n",
        "        if action == \"Arriba\":\n",
        "            self.player1 += abs(self.dy)\n",
        "        elif action == \"Abajo\":\n",
        "            self.player1 -= abs(self.dy)\n",
        "        self.avanza_player()\n",
        "        self.avanza_frame()\n",
        "        self.state = (floor(self.player1/abs(self.dy))-2, floor(self.y/abs(self.dy))-2, floor(self.x/abs(self.dx))-2)\n",
        "\n",
        "    def detectaColision(self, ball_y, player_y):\n",
        "        return (player_y+self.player_alto >= (ball_y-self.radio)) and (player_y <= (ball_y+self.radio))\n",
        "\n",
        "    def avanza_player(self):\n",
        "        if self.player1 + self.player_alto >= self.height_px:\n",
        "            self.player1 = self.height_px - self.player_alto\n",
        "        elif self.player1 <= -abs(self.dy):\n",
        "            self.player1 = -abs(self.dy)\n",
        "\n",
        "    def avanza_frame(self):\n",
        "        self.x += self.dx\n",
        "        self.y += self.dy\n",
        "        if self.x <= 3 or self.x > self.width_px:\n",
        "            self.dx = -self.dx\n",
        "            if self.x <= 3:\n",
        "                ret = self.detectaColision(self.y, self.player1)\n",
        "                if ret:\n",
        "                    self.score = 10\n",
        "                else:\n",
        "                    self.score = -10\n",
        "                    self.lives -= 1\n",
        "                    if self.lives > 0:\n",
        "                        self.x = randint(int(self.width_px/2), self.width_px)\n",
        "                        self.y = randint(0, self.height_px-10)\n",
        "                        self.dx = abs(self.dx)\n",
        "                        self.dy = abs(self.dy)\n",
        "        else:\n",
        "            self.score = 0\n",
        "        if self.y < 0 or self.y > self.height_px:\n",
        "            self.dy = -self.dy\n",
        "\n",
        "class PongAgent:\n",
        "    def __init__(self, game, policy=None, discount_factor=0.1, learning_rate=0.1, ratio_explotacion=0.9):\n",
        "        if policy is not None:\n",
        "            self._q_table = policy\n",
        "        else:\n",
        "            position = list(game.positions_space.shape)\n",
        "            position.append(len(game.action_space))\n",
        "            self._q_table = np.zeros(position)\n",
        "        self.discount_factor = discount_factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.ratio_explotacion = ratio_explotacion\n",
        "\n",
        "    def get_next_step(self, state, game):\n",
        "        next_step = np.random.choice(list(game.action_space))\n",
        "        if np.random.uniform() <= self.ratio_explotacion:\n",
        "            idx_action = np.random.choice(np.flatnonzero(\n",
        "                self._q_table[state[0], state[1], state[2]] ==\n",
        "                self._q_table[state[0], state[1], state[2]].max()\n",
        "            ))\n",
        "            next_step = list(game.action_space)[idx_action]\n",
        "        return next_step\n",
        "\n",
        "    def update(self, game, old_state, action_taken, reward_action_taken, new_state, reached_end):\n",
        "        idx_action_taken = list(game.action_space).index(action_taken)\n",
        "        actual_q_value = self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken]\n",
        "        future_q_value_options = self._q_table[new_state[0], new_state[1], new_state[2]]\n",
        "        future_max_q_value = reward_action_taken + self.discount_factor * future_q_value_options.max()\n",
        "        if reached_end:\n",
        "            future_max_q_value = reward_action_taken\n",
        "        self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken] = actual_q_value + \\\n",
        "            self.learning_rate * (future_max_q_value - actual_q_value)\n",
        "\n",
        "    def get_policy(self):\n",
        "        return self._q_table\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV9DuakYn0mo"
      },
      "source": [
        "##FUNCIÓN DE ENTRENAMIENTO\n",
        "En esta sección se define la función play(), que se encarga de entrenar al agente Pong usando el algoritmo Q-Learning.\n",
        "\n",
        "Lo que hace esta función es:\n",
        "\n",
        "Reiniciar el entorno y la posición de la pelota en cada partida.\n",
        "\n",
        "Permitir que el agente interactúe con el entorno durante muchas rondas (por ejemplo, 3000 o 5000 partidas).\n",
        "\n",
        "En cada paso, el agente elige una acción, recibe una recompensa y actualiza su tabla Q para mejorar su política.\n",
        "\n",
        "Controla la duración de las partidas y registra la recompensa total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7LlQJnWtnosf"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 2. FUNCIÓN DE ENTRENAMIENTO\n",
        "# ==========================\n",
        "def play(rounds=5000, max_life=3, discount_factor=0.1, learning_rate=0.1,\n",
        "         ratio_explotacion=0.9, learner=None, game=None, animate=False):\n",
        "    if game is None:\n",
        "        game = PongEnvironment(max_life=max_life, movimiento_px=3)\n",
        "    if learner is None:\n",
        "        learner = PongAgent(game, discount_factor=discount_factor, learning_rate=learning_rate, ratio_explotacion=ratio_explotacion)\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        state = game.reset()\n",
        "        done = False\n",
        "        while not done and game.total_reward <= 1000:\n",
        "            old_state = np.array(state)\n",
        "            next_action = learner.get_next_step(state, game)\n",
        "            state, reward, done = game.step(next_action, animate=animate)\n",
        "            learner.update(game, old_state, next_action, reward, state, done)\n",
        "    return learner, game\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE0usi89oHkb"
      },
      "source": [
        "## 3. ENTRENAR AGENTE Y MODELO KNN\n",
        "En esta sección se realiza todo el flujo de entrenamiento tanto del agente como del modelo supervisado K-NN:\n",
        "\n",
        "Entrenar al agente con Q-Learning utilizando la función play(). Esto permite generar una tabla Q con la política aprendida a partir de la experiencia.\n",
        "\n",
        "Extraer la Q-table y convertirla en un dataset supervisado, donde:\n",
        "\n",
        "Cada fila representa un estado del juego (posición del jugador, posición Y y X de la pelota).\n",
        "\n",
        "La etiqueta es la mejor acción para ese estado (según la política aprendida).\n",
        "\n",
        "Dividir el dataset en entrenamiento y validación.\n",
        "\n",
        "Entrenar un modelo K-Nearest Neighbors (KNN) con los datos generados, para que aprenda a imitar la política.\n",
        "\n",
        "Evaluar el modelo con métricas de precisión, recall y f1-score, verificando qué tan bien reproduce las decisiones del agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYltZtAhnsOY",
        "outputId": "34edc911-5513-42fe-914a-192ba13b9a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exactitud KNN: 0.6536731634182908\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.71      0.70       381\n",
            "           1       0.60      0.58      0.59       286\n",
            "\n",
            "    accuracy                           0.65       667\n",
            "   macro avg       0.65      0.64      0.65       667\n",
            "weighted avg       0.65      0.65      0.65       667\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# 3. ENTRENAR AGENTE Y MODELO KNN\n",
        "# ==========================\n",
        "learner, game = play(rounds=3000, discount_factor=0.1, learning_rate=0.1, ratio_explotacion=0.9)\n",
        "\n",
        "q_table = learner.get_policy()\n",
        "pos_n, y_n, x_n, n_actions = q_table.shape\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i in range(pos_n):\n",
        "    for j in range(y_n):\n",
        "        for k in range(x_n):\n",
        "            best_action_idx = int(np.argmax(q_table[i, j, k]))\n",
        "            X.append([i, j, k])\n",
        "            y.append(best_action_idx)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_val)\n",
        "print(\"Exactitud KNN:\", accuracy_score(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBnwXUC6h6Gs"
      },
      "source": [
        "##Análisis de resultados — K-NN\n",
        "\n",
        "El modelo K-Nearest Neighbors alcanzó una exactitud del 65,8 %, con una precisión y recall equilibrados entre ambas clases de acción (Arriba y Abajo). La clase 0 (Arriba) presentó un desempeño ligeramente superior (precisión 0.70), lo que indica que el clasificador identifica mejor las situaciones en las que la acción correcta es moverse hacia arriba.\n",
        "\n",
        "Estos resultados son razonables, considerando que el espacio de estados del entorno Pong es discreto y que K-NN utiliza una métrica de vecindad simple sin ponderaciones adicionales. El modelo logra capturar de forma aproximada la política aprendida por Q-Learning, aunque con un margen de error visible que podría deberse a estados menos frecuentes en el entrenamiento o a acciones ambiguas cerca de los bordes.\n",
        "\n",
        "En resumen, el K-NN constituye un modelo sustituto funcional, capaz de reproducir la política con una precisión aceptable, ofreciendo simplicidad y rapidez sin requerir un entrenamiento complejo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
